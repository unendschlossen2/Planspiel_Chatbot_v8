# config.yaml
# true/false Werte sind in Kleinbuchstaben geschrieben.

models:
  embedding_id: "infly/inf-retriever-v1-1.5b"
  reranker_id: "BAAI/bge-reranker-large"

  # --- Haupt-LLM für die Antwortgenerierung ---
  llm_model_id: "local-llm-v1"
  llm_model_path: "path/to/your/main_model.gguf"
  llm_generation_config:
    temperature: 0.2
    max_tokens: 2048
    n_ctx: 4096
    n_gpu_layers: -1
    verbose: false

  # --- Konversations-Verdichter (Condenser) ---
  condenser_model_path: "path/to/your/condenser_model.gguf"
  condenser_generation_config:
    temperature: 0.0
    max_tokens: 256
    n_ctx: 2048
    n_gpu_layers: -1
    verbose: false

  # --- Anfrage-Erweiterer (Query Expander) ---
  query_expander_model_path: "path/to/your/expander_model.gguf"
  query_expander_generation_config:
    temperature: 0.7
    max_tokens: 128
    n_ctx: 2048
    n_gpu_layers: -1
    verbose: false

# Einstellungen für den ChromaDB Vektorspeicher
database:
  persist_path: "DATABASE_STORAGE/chroma_vector_db"
  collection_name: "topsim_gm_coll_gemma3_test"
  force_rebuild: False

# Parameter für die Datenverarbeitung und Chunking-Logik
processing:
  initial_split_level: 3
  max_chars_per_chunk: 2100
  min_chars_per_chunk: 75

# Parameter zur Feinabstimmung der Retrieval-Augmented Generation (RAG) Pipeline
pipeline:
  enable_conversation_memory: true
  use_reranker: true
  enable_query_expansion: true
  query_expansion_char_threshold: 25
  retrieval_top_k: 15
  default_retrieval_top_k: 3
  min_chunks_to_llm: 1
  max_chunks_to_llm: 5
  min_absolute_score_threshold: 0.001
  min_chunks_for_gap_detection: 4
  gap_detection_factor: 0.25
  small_epsilon: 0.00001

# --- System und Performance ---
system:
  low_vram_mode: true