# config.yaml
# true/false Werte sind in Kleinbuchstaben geschrieben.

models:
  embedding_id: "infly/inf-retriever-v1-1.5b"
  reranker_id: "BAAI/bge-reranker-large"

  # --- NEU: Einstellungen für das lokale LLM (llama-cpp-python) ---
  llm_model_id: "local-llm-v1" # Ein Name für das Modell
  # WICHTIG: Ersetzen Sie dies durch den tatsächlichen Pfad zu Ihrer GGUF-Modelldatei
  llm_model_path: "path/to/your/model.gguf"

  query_expander_id: "query-expander-v1"
  condenser_model_id: "condenser-v1"

  # --- NEU: Generierungsoptionen für das lokale LLM ---
  llm_generation_config:
    temperature: 0.2
    max_tokens: 2048 # Ersetzt num_predict von Ollama
    n_ctx: 4096 # Kontextgröße des Modells
    n_gpu_layers: -1 # -1, um alle möglichen Schichten auf die GPU auszulagern
    verbose: false # Ob Llama-cpp-Python detaillierte Logs ausgeben soll

# Einstellungen für den ChromaDB Vektorspeicher
database:
  persist_path: "DATABASE_STORAGE/chroma_vector_db"
  collection_name: "topsim_gm_coll_gemma3_test"
  # Auf True setzen, um beim Start eine vollständige Datenverarbeitung und Neuerstellung der Datenbank zu erzwingen
  force_rebuild: False

# Parameter für die Datenverarbeitung und Chunking-Logik
processing:
  initial_split_level: 3
  max_chars_per_chunk: 2100
  min_chars_per_chunk: 75

# Parameter zur Feinabstimmung der Retrieval-Augmented Generation (RAG) Pipeline
pipeline:
  enable_conversation_memory: true
  use_reranker: true
  enable_query_expansion: true
  query_expansion_char_threshold: 25
  retrieval_top_k: 15
  default_retrieval_top_k: 3
  min_chunks_to_llm: 1
  max_chunks_to_llm: 5
  min_absolute_score_threshold: 0.001
  min_chunks_for_gap_detection: 4
  gap_detection_factor: 0.25
  small_epsilon: 0.00001

# --- System und Performance ---
system:
  low_vram_mode: true